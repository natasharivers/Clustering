{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mall Customers Exercises\n",
    "\n",
    "## notebook\n",
    "\n",
    "1. Acquire data from mall_customers.customers in mysql database.\n",
    "2. Summarize data (include distributions and descriptive statistics).\n",
    "3. Detect outliers using IQR.\n",
    "4. Split data (train, validate, and test split).\n",
    "5. Encode categorical columns using a one hot encoder (pd.get_dummies).\n",
    "6. Handles missing values.\n",
    "7. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrangle_mall.py\n",
    "\n",
    "1. Acquire data from mall_customers.customers in mysql database.\n",
    "2. Split the data into train, validate, and split\n",
    "3. One-hot-encoding (pd.get_dummies)\n",
    "4. Missing values\n",
    "5. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pydataset import data\n",
    "import scipy.stats as stats\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "import acquire\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Acquire data from mall_customers.customers in mysql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function in acquire.py\n",
    "def get_connection(db_name):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    from env import host, user, password\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function in acquire.py\n",
    "def get_mall_customers():\n",
    "    '''\n",
    "    This function reads in the mall_customers data from the Codeup db\n",
    "    returns: a pandas DataFrame \n",
    "    '''\n",
    "    \n",
    "    mall_query = '''\n",
    "    SELECT *\n",
    "    FROM customers\n",
    "    '''\n",
    "    return pd.read_sql(mall_query, get_connection('mall_customers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_mall_customers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Summarize data (include distributions and descriptive statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    '''\n",
    "    this function will take in a single argument (a pandas df) \n",
    "    output to console various statistics on said dataframe, including:\n",
    "    #.head()\n",
    "    #.info()\n",
    "    #.describe()\n",
    "    #.value_counts()\n",
    "    #observation of nulls in the dataframe\n",
    "    '''\n",
    "    #print head\n",
    "    print('=================================================')\n",
    "    print('Dataframe head: ')\n",
    "    print(df.head(3))\n",
    "    \n",
    "    #print info\n",
    "    print('=================================================')\n",
    "    print('Dataframe info: ')\n",
    "    print(df.info())\n",
    "    \n",
    "    #print descriptive stats\n",
    "    print('=================================================')\n",
    "    print('DataFrame Description')\n",
    "    print(df.describe())\n",
    "    num_cols = df.select_dtypes(exclude='O').columns.to_list()\n",
    "    cat_cols = df.select_dtypes(include='O').columns.to_list()\n",
    "    \n",
    "    #print value counts\n",
    "    print('=================================================')\n",
    "    print('Dataframe value counts: ')\n",
    "    for col in df. columns:\n",
    "        if col in cat_cols:\n",
    "            print(df[col].value_counts())\n",
    "        else:\n",
    "            print(df[col].value_counts(bins=10, sort = False))\n",
    "    \n",
    "    #print nulls by column\n",
    "    print('=================================================')\n",
    "    print('nulls in dataframe by column: ')\n",
    "    print(nulls_by_col(df))\n",
    "    \n",
    "    #print nulls by column\n",
    "    print('=================================================')\n",
    "    print('nulls in dataframe by row: ')\n",
    "    print(nulls_by_row(df))\n",
    "    print('=================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.summarize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Detect outliers using IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from wrangle.py\n",
    "def outlier_bound_calculation(df, variable):\n",
    "    '''\n",
    "    This function calcualtes the lower and upper bound \n",
    "    to locate outliers in the variable named\n",
    "    '''\n",
    "    quartile1, quartile3 = np.percentile(df[variable], [25,75])\n",
    "    IQR_value = quartile3 - quartile1\n",
    "    lower_bound = quartile1 - (1.5 * IQR_value)\n",
    "    upper_bound = quartile3 + (1.5 * IQR_value)\n",
    "    '''\n",
    "    returns the lowerbound and upperbound values\n",
    "    '''\n",
    "    return print(f'For {variable} the lower bound is {lower_bound} and  upper bound is {upper_bound}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_outliers = wrangle.outlier_bound_calculation(df, 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_outliers = wrangle.outlier_bound_calculation(df, 'annual_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_outliers = wrangle.outlier_bound_calculation(df, 'spending_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Split data (train, validate, and test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from wrangle.py\n",
    "def zillow_split(df, target):\n",
    "    '''\n",
    "    This function take in get_zillow  from aquire.py and performs a train, validate, test split\n",
    "    Returns train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "    and prints out the shape of train, validate, test\n",
    "    '''\n",
    "    #create train_validate and test datasets\n",
    "    train, test = train_test_split(df, train_size = 0.8, random_state = 123)\n",
    "    #create train and validate datasets\n",
    "    train, validate = train_test_split(train, train_size = 0.7, random_state = 123)\n",
    "\n",
    "    #Split into X and y\n",
    "    X_train = train.drop(columns=[target])\n",
    "    y_train = train[target]\n",
    "\n",
    "    X_validate = validate.drop(columns=[target])\n",
    "    y_validate = validate[target]\n",
    "\n",
    "    X_test = test.drop(columns=[target])\n",
    "    y_test = test[target]\n",
    "\n",
    "    # Have function print datasets shape\n",
    "    print(f'train -> {train.shape}')\n",
    "    print(f'validate -> {validate.shape}')\n",
    "    print(f'test -> {test.shape}')\n",
    "   \n",
    "    return train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test, X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle.zillow_split(df, 'logerror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #5 Encode categorical columns using a one hot encoder (pd.get_dummies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, cols, drop_first=True):\n",
    "    '''\n",
    "    Take in df and list of columns\n",
    "    add encoded columns derived from columns in list to the df\n",
    "    '''\n",
    "    for col in cols:\n",
    "\n",
    "        dummies = pd.get_dummies(df[f'{col}'], drop_first=drop_first) # get dummy columns\n",
    "\n",
    "        df = pd.concat([df, dummies], axis=1) # add dummy columns to df\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encoding(train, ['gender'], drop_first=True)\n",
    "validate = encoding(validate, ['gender'], drop_first=True)\n",
    "test = encoding(test, ['gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #6 Handles missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are no missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #7 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(train, validate, test, num_cols):\n",
    "    '''\n",
    "    This function adds scaled versions of a list of columns \n",
    "    to train, validate, and test\n",
    "    '''\n",
    "    \n",
    "    # reset index for merge \n",
    "    train = train.reset_index(drop=True)\n",
    "    validate = validate.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    scaler = sklearn.preprocessing.MinMaxScaler() # create scaler object\n",
    "\n",
    "    scaler.fit(train[num_cols]) # fit the object \n",
    "\n",
    "    # transform to get scaled columns\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train[num_cols]), columns = train[num_cols].columns + \"_scaled\")\n",
    "    validate_scaled = pd.DataFrame(scaler.transform(validate[num_cols]), columns = validate[num_cols].columns + \"_scaled\")\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test[num_cols]), columns = test[num_cols].columns + \"_scaled\")\n",
    "    \n",
    "    # add scaled columns to dataframes\n",
    "    train = train.merge(train_scaled, left_index=True, right_index=True)\n",
    "    validate = validate.merge(validate_scaled, left_index=True, right_index=True)\n",
    "    test = test.merge(train_scaled, left_index=True, right_index=True)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrangle_mall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(database, user=user, host=host, password=password):\n",
    "    '''get URL with user, host, and password from env '''\n",
    "    \n",
    "    return f\"mysql+pymysql://{user}:{password}@{host}/{database}\"\n",
    "    \n",
    "\n",
    "def get_sql_data(database,query):\n",
    "    ''' \n",
    "        Take in a database and query\n",
    "        check if csv exists for the queried database\n",
    "        if it does read from the csv\n",
    "        if it does not create the csv then read from the csv  \n",
    "    '''\n",
    "    \n",
    "    if os.path.isfile(f'{database}_query.csv') == False:   # check for the file\n",
    "        \n",
    "        df = pd.read_sql(query, get_connection(database))  # create file \n",
    "        \n",
    "        df.to_csv(f'{database}_query.csv',index = False)   # cache file\n",
    "        \n",
    "    return pd.read_csv(f'{database}_query.csv') # return contents of file\n",
    "\n",
    "\n",
    "def get_mall_data():\n",
    "    ''' acquire data from mall_customers database'''\n",
    "    \n",
    "    database = \"mall_customers\"\n",
    "\n",
    "    query = \"select * from customers\"\n",
    "\n",
    "    df = get_sql_data(database,query)\n",
    "    \n",
    "    return df\n",
    "\n",
    "##################################Prepare##########################################\n",
    "\n",
    "def detect_outliers(df, k, col_list, remove=False):\n",
    "    ''' get upper and lower bound for list of columns in a dataframe \n",
    "        if desired return that dataframe with the outliers removed\n",
    "    '''\n",
    "    \n",
    "    odf = pd.DataFrame()\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q2, q3 = df[f'{col}'].quantile([.25, .5, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "        \n",
    "        # print each col and upper and lower bound for each column\n",
    "        print(f\"{col}: Median = {q2} lower_bound = {lower_bound} upper_bound = {upper_bound}\")\n",
    "\n",
    "        # return dataframe of outliers\n",
    "        odf = odf.append(df[(df[f'{col}'] < lower_bound) | (df[f'{col}'] > upper_bound)])\n",
    "            \n",
    "    return odf\n",
    "\n",
    "\n",
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[f'{col}'].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        return df[(df[f'{col}'] > lower_bound) & (df[f'{col}'] < upper_bound)]  \n",
    "    \n",
    "    \n",
    "def train_validate_test_split(df):\n",
    "    '''split df into train, validate, test'''\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123)\n",
    "    \n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "def min_max_scaling(train, validate, test, num_cols):\n",
    "    '''\n",
    "    Add scaled versions of a list of columns to train, validate, and test\n",
    "    '''\n",
    "    \n",
    "    # reset index for merge \n",
    "    train = train.reset_index(drop=True)\n",
    "    validate = validate.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    scaler = sklearn.preprocessing.MinMaxScaler() # create scaler object\n",
    "\n",
    "    scaler.fit(train[num_cols]) # fit the object \n",
    "\n",
    "    # transform to get scaled columns\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train[num_cols]), columns = train[num_cols].columns + \"_scaled\")\n",
    "    validate_scaled = pd.DataFrame(scaler.transform(validate[num_cols]), columns = validate[num_cols].columns + \"_scaled\")\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test[num_cols]), columns = test[num_cols].columns + \"_scaled\")\n",
    "    \n",
    "    # add scaled columns to dataframes\n",
    "    train = train.merge(train_scaled, left_index=True, right_index=True)\n",
    "    validate = validate.merge(validate_scaled, left_index=True, right_index=True)\n",
    "    test = test.merge(train_scaled, left_index=True, right_index=True)\n",
    "    \n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "def prepare_mall_data(df):\n",
    "    ''' prepare mall data'''\n",
    "    \n",
    "    # split data\n",
    "    train, validate, test = train_validate_test_split(df) \n",
    "       \n",
    "    # encode gender in each column\n",
    "    train = encoding(train, ['gender'], drop_first=True)\n",
    "    validate = encoding(validate, ['gender'], drop_first=True)\n",
    "    test = encoding(test, ['gender'], drop_first=True)\n",
    "    \n",
    "    # scale age, annual_income, and spending_score\n",
    "    train, validate, test = min_max_scaling(train, validate, test,  ['age', 'annual_income', 'spending_score'])\n",
    "    \n",
    "    return train, validate, test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
